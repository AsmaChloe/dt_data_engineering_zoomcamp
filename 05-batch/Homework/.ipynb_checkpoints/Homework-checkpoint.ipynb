{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada15bab-c2a9-4a40-a755-446ddb0680dd",
   "metadata": {},
   "source": [
    "### Question 1: \r\n",
    "\r\n",
    "**Install Spark and PySpark** \r\n",
    "\r\n",
    "- Install Spark\r\n",
    "- Run PySpark\r\n",
    "- Create a local spark session\r\n",
    "- Execute spark.version.\r\n",
    "\r\n",
    "What's the output?\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef4e311-a52a-4b70-9451-56ba7afd474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa7a8bb-fbcb-49cd-8c8c-4248bab47ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17ed62-73cf-4405-be14-e819ef35ce81",
   "metadata": {},
   "source": [
    "### Question 2: \r\n",
    "\r\n",
    "**FHV October 2019**\r\n",
    "\r\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\r\n",
    "\r\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\r\n",
    "\r\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e53fa6-032e-47bf-bd2a-2fb3732f52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! download_fhv_data.sh fhv 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29574524-7e9c-4a27-a411-2a5b8390685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv('data/raw/fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d853433-aa16-4471-9f59-64b95b309398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26485fc0-59c8-48bd-a1b0-6c2f29d1a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "fhv_schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True), \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2329b1e-817d-444c-bfc3-54a502d40fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(fhv_schema) \\\n",
    "        .csv('data/raw/fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b123b95-a50c-4d53-9fe5-09a20ae361b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = df_fhv.repartition(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e209c06-eb8e-4312-b776-17c80f9163ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.write.parquet('fhvhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2383ad55-fce4-4a25-ae6c-73e4a22f9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_parquet = spark.read.parquet('fhvhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09d99971-7f6d-4244-9720-ba8d779ae4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C n'a pas de nom.\n",
      " Le num‚ro de s‚rie du volume est 144C-EFC3\n",
      "\n",
      " R‚pertoire de C:\\Users\\asma-\\Documents\\Dev\\dt_data_engineering_zoomcamp\\05-batch\\Homework\\fhvhv\\2019\\10\n",
      "\n",
      "04/03/2024  20:10           6664698 part-00000-76180920-6c87-4dc1-8a63-33b1f806da02-c000.snappy.parquet\n",
      "04/03/2024  20:10           6657082 part-00001-76180920-6c87-4dc1-8a63-33b1f806da02-c000.snappy.parquet\n",
      "04/03/2024  20:10           6665057 part-00002-76180920-6c87-4dc1-8a63-33b1f806da02-c000.snappy.parquet\n",
      "04/03/2024  20:10           6665082 part-00003-76180920-6c87-4dc1-8a63-33b1f806da02-c000.snappy.parquet\n",
      "04/03/2024  20:10           6665016 part-00004-76180920-6c87-4dc1-8a63-33b1f806da02-c000.snappy.parquet\n",
      "04/03/2024  20:10           6662655 part-00005-76180920-6c87-4dc1-8a63-33b1f806da02-c000.snappy.parquet\n",
      "               6 fichier(s)         39979590 octets\n",
      "\n",
      "     Total des fichiers list‚sÿ:\n",
      "               6 fichier(s)         39979590 octets\n",
      "               0 R‚p(s)    140500303872 octets libres\n"
     ]
    }
   ],
   "source": [
    "!dir \"fhvhv\\2019\\10\\*.parquet\" /s /-c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e778413-7c02-45f1-be93-59175e885c19",
   "metadata": {},
   "source": [
    "### Question 3: \r\n",
    "\r\n",
    "**Count records** \r\n",
    "\r\n",
    "How many taxi trips were there on the 15th of October?\r\n",
    "\r\n",
    "Consider only trips that started on the 15th of Oc0\r\n",
    "- 62,610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4f2e8e0-f7f7-4479-8bdd-f705a7b2228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_fhv_parquet.filter(F.to_date(df_fhv_parquet.pickup_datetime) == F.lit('2019-10-15')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b241d-2925-4c83-a959-27615ef6da77",
   "metadata": {},
   "source": [
    "### Question 4: \r\n",
    "\r\n",
    "**Longest trip for each day** \r\n",
    "\r\n",
    "What is the length of the longest trip in the dataset in hors?\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9402b300-145c-4c5f-86a7-a9c5ee9f4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_fhv_parquet.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4849876c-598e-4540-870c-e614ce09a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        pickup_datetime, \n",
    "        MAX(TIMESTAMPDIFF(HOUR, pickup_datetime, dropOff_datetime)) AS hour_difference\n",
    "    FROM trips_data\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fbbbdb26-7911-47ab-afc3-21f526995829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "|    pickup_datetime|hour_difference|\n",
      "+-------------------+---------------+\n",
      "|2019-10-28 09:00:00|         631152|\n",
      "|2019-10-11 18:00:00|         631152|\n",
      "|2019-10-31 23:46:33|          87672|\n",
      "|2019-10-01 21:43:42|          70128|\n",
      "|2019-10-17 14:00:00|           8794|\n",
      "|2019-10-26 21:26:00|           8784|\n",
      "|2019-10-30 12:30:04|           1464|\n",
      "|2019-10-25 07:04:57|           1056|\n",
      "|2019-10-01 07:21:12|            793|\n",
      "|2019-10-01 13:47:17|            793|\n",
      "|2019-10-01 13:41:00|            793|\n",
      "|2019-10-01 06:04:13|            792|\n",
      "|2019-10-01 06:54:57|            792|\n",
      "|2019-10-01 10:24:04|            792|\n",
      "|2019-10-01 12:37:49|            792|\n",
      "|2019-10-01 09:06:55|            792|\n",
      "|2019-10-01 14:55:36|            792|\n",
      "|2019-10-01 17:39:45|            792|\n",
      "|2019-10-01 05:41:00|            792|\n",
      "|2019-10-01 12:02:44|            792|\n",
      "+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f17307-3c0c-45ae-89a8-f2a73c02b0ef",
   "metadata": {},
   "source": [
    "### Question 5: \r\n",
    "\r\n",
    "**User Interface**\r\n",
    "\r\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local po\n",
    "- 4040\r\n",
    "rt?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db85830-d4d2-4ed7-842a-a1364b8dc8be",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "\n",
    "**Least frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark</br>\n",
    "[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8975ab1e-809b-43d9-95d6-24dfc3a7be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "67d64780-b8bb-4a40-a1f0-bc2a147bdabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('LocationID', StringType(), True), StructField('Borough', StringType(), True), StructField('Zone', StringType(), True), StructField('service_zone', StringType(), True)])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0624f14b-0f96-4d0b-8d24-2dae7eabe379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "80a7f847-2f82-43b8-9bbc-b269c33eb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        Zone, \n",
    "        count(*)\n",
    "    FROM trips_data\n",
    "    INNER JOIN zones\n",
    "    on LocationID = PULocationID\n",
    "    group by 1\n",
    "    order by 2 asc\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "04dadc4a-87fc-4ff1-9656-286fcd2a5e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                Zone|count(1)|\n",
      "+--------------------+--------+\n",
      "|         Jamaica Bay|       1|\n",
      "|Governor's Island...|       2|\n",
      "| Green-Wood Cemetery|       5|\n",
      "|       Broad Channel|       8|\n",
      "|     Highbridge Park|      14|\n",
      "|        Battery Park|      15|\n",
      "|Saint Michaels Ce...|      23|\n",
      "|Breezy Point/Fort...|      25|\n",
      "|Marine Park/Floyd...|      26|\n",
      "|        Astoria Park|      29|\n",
      "|    Inwood Hill Park|      39|\n",
      "|       Willets Point|      47|\n",
      "|Forest Park/Highl...|      53|\n",
      "|  Brooklyn Navy Yard|      57|\n",
      "|        Crotona Park|      62|\n",
      "|        Country Club|      77|\n",
      "|     Freshkills Park|      89|\n",
      "|       Prospect Park|      98|\n",
      "|     Columbia Street|     105|\n",
      "|  South Williamsburg|     110|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
